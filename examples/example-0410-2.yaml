# Example configuration for YART (Your Another Reranker Trainer)

model_args:
  model_name_or_path: "hotchpotch/mMiniLMv2-L6-H384"
  classifier_dropout: 0.0
  trust_remote_code: false

data_args:
  train_data:
    -
      dataset_class: "yart.custom_dataset.jlawir.JLaWIRDataset"
      dataset_options:
        jlawir_ds_path: "/home/hotchpotch/src/github.com/hotchpotch/lm-trainers/query-gen/japanese-ir-qa-large/dataset/20250330/hq/train/"
        jlawir_score_ds_path: "/home/hotchpotch/src/github.com/hotchpotch/lm-trainers/query-gen/japanese-ir-qa-large/hard_negs_hq/dataset/production/"
        train_size: 100000
        test_size: 1000
        max_length: 512
        pick_top_100: 12
        slice_top_100_k: 20

training_args:
  output_dir: "./outputs/reranker-model-example"
  overwrite_output_dir: true
  
  # Training parameters
  learning_rate: 5.0e-4
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 32
  train_group_size: 16
  max_grad_norm: 0.3
  
  # Optimizer and scheduler
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # Loss function
  loss_name: "cross_entropy"  # Options: cross_entropy, mse, margin_ranking_loss
  
  # Logging and saving
  logging_steps: 1 # debug
  save_steps: 100000  # Set to a large number to disable intermediate saving
  save_total_limit: 2
  
  # Mixed precision
  fp16: false
  bf16: true
  
  # Other settings
  dataloader_num_workers: 8
  load_best_model_at_end: false
  num_train_epochs: 1
  
  # Reporting
  report_to: ["wandb"]
  
run_args:
  debug: false
  output_prefix: ""
  remove_checkpoints: true